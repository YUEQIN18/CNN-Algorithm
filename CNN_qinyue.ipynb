{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain \n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "def get_stopwords():\n",
    "    with open('./data/stopwords.txt',encoding='utf-8') as f:  \n",
    "        stopwords = list(set(f.read().splitlines()))\n",
    "    return stopwords\n",
    "\n",
    "def get_punctuations():\n",
    "    with open('./data/punctuations.txt',encoding='utf-8') as f:\n",
    "        punctuations = list(set(f.read().splitlines()))\n",
    "    return punctuations\n",
    "\n",
    "def clean_text(text): # Clean review text\n",
    "    text = re.sub(r\"[^A-Za-z]\", \" \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \\'s\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" \\'ve\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" n\\'t\", text)\n",
    "    text = re.sub(r\"\\'re\", \" \\'re\", text)\n",
    "    text = re.sub(r\"\\'d\", \" \\'d\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" \\'ll\", text)\n",
    "    text = re.sub(r\",\", \" , \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\(\", \" \\( \", text)\n",
    "    text = re.sub(r\"\\)\", \" \\) \", text)\n",
    "    text = re.sub(r\"\\?\", \" \\? \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "    stop_words = get_stopwords()\n",
    "    punctuations = get_punctuations()\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, ' ')  # Use spaces instead of punctuation marks\n",
    "    word_list = WordPunctTokenizer().tokenize(text)  \n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    return word_list\n",
    "   \n",
    "def user_item_reviews(x):\n",
    "    ur = user_reviews.loc[x[\"userID\"]].values.tolist()# get all user review\n",
    "    ir = item_reviews.loc[x[\"itemID\"]].values.tolist()# get all item review\n",
    "    x[\"user_reviews\"] = \" \".join(list(chain(*list(chain(*ur))))[:50])\n",
    "    x[\"item_reviews\"] = \" \".join(list(chain(*list(chain(*ir))))[:50])\n",
    "    return x\n",
    "\n",
    "qy_data_o = pd.read_csv('./data/Home_and_Kitchen.csv')\n",
    "qy_data_o.columns = ['userID','itemID','ratings','reviews']\n",
    "qy_data_p = qy_data_o.copy()\n",
    "qy_data_p = qy_data_p.drop(qy_data_p[[not isinstance(x, str) or len(x) == 0 for x in qy_data_p['reviews']]].index)#Remove empty text\n",
    "user_num_count = qy_data_p[['userID', 'ratings']].groupby('userID', as_index=False).size()#Count the number of userIDs\n",
    "item_num_count = qy_data_p[['itemID', 'ratings']].groupby('itemID', as_index=False).size()#Count the number of itemIDs\n",
    "\n",
    "user_num_unique = user_num_count.index#get userID\n",
    "item_num_unique = item_num_count.index#get itemID\n",
    "user_id_dict = dict((sid, i) for (i, sid) in enumerate(user_num_unique))#Renumber userID, from 0 to n, build mapping\n",
    "item_id_dict = dict((uid, i) for (i, uid) in enumerate(item_num_unique))#Renumber itemID, from 0 to n, build mapping\n",
    "qy_data_p['userID'] = qy_data_p['userID'].map(lambda x:user_id_dict[x])#Renumber userID, from 0 to n\n",
    "qy_data_p['itemID'] = qy_data_p['itemID'].map(lambda x:item_id_dict[x])#Renumber itemID, from 0 to n\n",
    "qy_data_p['reviews'] = qy_data_p['reviews'].apply(clean_text)#clean reviews\n",
    "\n",
    "user_reviews = pd.pivot_table(qy_data_p,\n",
    "                              index=[\"userID\", \"itemID\"],\n",
    "                              aggfunc=lambda x: x).drop(\"ratings\", axis=1)#Aggregate function\n",
    "item_reviews = pd.pivot_table(qy_data_p,\n",
    "                               index=[\"itemID\", \"userID\"],\n",
    "                               aggfunc=lambda x: x).drop(\"ratings\", axis=1)#Aggregate function\n",
    "user_item_rating = qy_data_p.filter(regex='userID|itemID|ratings')#Filter review text\n",
    "qy_data_cleaned = user_item_rating.apply(user_item_reviews, axis=1)#Get item-review, and user-review, get the final cleaned data\n",
    "qy_data_cleaned.to_csv(\"./data/qy_data_cleaned.csv\", index=False)#save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten,Dropout\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras\n",
    "\n",
    "class CNNRecommend():\n",
    "    def __init__(self,embedding_size,user_seq_len,item_seq_len):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.user_input_layer, self.user_model = self.build_cnn_model(user_seq_len)#user cnn\n",
    "        self.item_input_layer, self.item_model = self.build_cnn_model(item_seq_len)#item CNN\n",
    "        self.combine = Concatenate()([self.user_model, self.item_model])#CNN stitching users and items\n",
    "        self.model_out = Dense(6,activation='softmax')(self.combine)#The last layer,predict classification\n",
    "    #Compile the CNN model\n",
    "    def build_cnn_model(self, max_seq_len):\n",
    "        input_layer = Input(shape=(max_seq_len, self.embedding_size))\n",
    "        model = Conv1D(128, 5, padding='same',activation=\"relu\")(input_layer)#\n",
    "        model = MaxPooling1D(3, 3, padding='same')(model)\n",
    "        model = Conv1D(64, 5, padding='same')(model)\n",
    "        model = Flatten()(model)\n",
    "        model = Dropout(0.5)(model)\n",
    "        model = BatchNormalization()(model)\n",
    "        model = Dense(128,activation=\"relu\")(model)\n",
    "        return input_layer, model\n",
    "    #Build model\n",
    "    def create_cnn_model(self):\n",
    "        output = self.model_out\n",
    "        self.model = Model(inputs=[self.user_input_layer, self.item_input_layer], outputs=[output])\n",
    "        adam = keras.optimizers.Adam(lr = 0.005, beta_1=0.95, beta_2=0.999,epsilon=1e-08)#优化器\n",
    "        self.model.compile(optimizer=adam, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "     #train\n",
    "    def train(self,user_reviews,item_reviews,ratings_data, epochs=5):\n",
    "        self.create_cnn_model()#\n",
    "        self.train_inputs = [user_reviews, item_reviews]\n",
    "        self.train_outputs = to_categorical(ratings_data) \n",
    "        \n",
    "        self.history = self.model.fit(self.train_inputs,\n",
    "                                      self.train_outputs,\n",
    "                                      validation_split=0.1,\n",
    "                                      batch_size=32,\n",
    "                                      epochs=epochs)\n",
    "    def evaluate(self,user_reviews,item_reviews,ratings_data):\n",
    "        self.test_inputs = [user_reviews, item_reviews]\n",
    "        self.test_outputs = to_categorical(ratings_data)\n",
    "        scores = self.model.evaluate(self.test_inputs, self.test_outputs, verbose=0)\n",
    "        print('test accuracy:%.2f%%' % ( scores[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 35993 samples, validate on 4000 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "35993/35993 [==============================] - 10s 288us/step - loss: 0.9817 - acc: 0.6874 - val_loss: 0.9397 - val_acc: 0.7033\n",
      "Epoch 2/5\n",
      "35993/35993 [==============================] - 9s 259us/step - loss: 0.8793 - acc: 0.7026 - val_loss: 0.8519 - val_acc: 0.7065\n",
      "Epoch 3/5\n",
      "35993/35993 [==============================] - 10s 265us/step - loss: 0.8439 - acc: 0.7092 - val_loss: 0.8327 - val_acc: 0.7107\n",
      "Epoch 4/5\n",
      "35993/35993 [==============================] - 9s 253us/step - loss: 0.8156 - acc: 0.7139 - val_loss: 0.8547 - val_acc: 0.6980\n",
      "Epoch 5/5\n",
      "35993/35993 [==============================] - 9s 247us/step - loss: 0.7845 - acc: 0.7218 - val_loss: 0.8373 - val_acc: 0.7013\n",
      "test accuracy:70.92%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import CNNRecommend\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Convert each user’s comment into a digital vector, and get the word vector through GloVe\n",
    "def glove_and_pad(item_seq_len, user_seq_len, pad_value, glove_map):\n",
    "    def embed(row):\n",
    "        sentence = str(row[\"user_reviews\"]).split()[:user_seq_len]\n",
    "        reviews = list(map(lambda word: glove_map.get(word)\n",
    "            if word in glove_map else pad_value, sentence))\n",
    "        row[\"user_reviews\"] = reviews +\\\n",
    "                [pad_value] * (user_seq_len - len(reviews))\n",
    "        sentence = str(row[\"item_reviews\"]).split()[:item_seq_len]\n",
    "        reviews = list(map(lambda word: glove_map.get(word)\n",
    "            if word in glove_map else pad_value, sentence))\n",
    "        row[\"item_reviews\"] = reviews +\\\n",
    "                [pad_value] * (item_seq_len - len(reviews))\n",
    "        return row\n",
    "    return embed\n",
    "#GloVe word vector mapping, dictionary type\n",
    "def glove_map():\n",
    "    with open('./data/glove.6B.50d.txt',encoding='utf-8') as fs:\n",
    "        return {l[0]: np.asarray(l[1:], dtype=\"float32\") for l in [line.split() for line in fs]}\n",
    "qy_data_cleaned = pd.read_csv(\"./data/qy_data_cleaned.csv\")\n",
    "user_seq_sizes = qy_data_cleaned.loc[:, \"user_reviews\"].apply(lambda x: str(x).split()).apply(len)\n",
    "item_seq_sizes = qy_data_cleaned.loc[:, \"item_reviews\"].apply(lambda x: str(x).split()).apply(len)\n",
    "user_ptile = 40\n",
    "item_ptile = 15\n",
    "emb_size = 50\n",
    "user_seq_len = int(np.percentile(user_seq_sizes, user_ptile))#Get the length of the user review text sequence\n",
    "item_seq_len = int(np.percentile(item_seq_sizes, item_ptile))#Get the length of the item review text sequence\n",
    "\n",
    "g2v_map = glove_map()#gloVe word vector dictionary mapping\n",
    "glove_fn = glove_and_pad(item_seq_len, user_seq_len, np.array([0.0] * emb_size), g2v_map)#glove\n",
    "train_data,test_data = train_test_split(hak_data_cleaned,test_size = 0.2,random_state=42) \n",
    "train_data_p = train_data.apply(glove_fn, axis=1)\n",
    "\n",
    "train_rating = train_data_p.loc[:, \"ratings\"].values#Y value of training set\n",
    "train_user_reviews = np.array(list(train_data_p.loc[:, \"user_reviews\"]))#The user_reviews feature matrix of the training set\n",
    "train_item_reviews = np.array(list(train_data_p.loc[:, \"item_reviews\"]))#The item_reviews feature matrix of the training set\n",
    "\n",
    "test_data_p= test_data.apply(glove_fn, axis=1)#test\n",
    "test_rating = test_data_p.loc[:, \"ratings\"].values#test ratings\n",
    "test_user_reviews = np.array(list(test_data_p.loc[:, \"user_reviews\"]))#test user_reviews\n",
    "test_item_reviews = np.array(list(test_data_p.loc[:, \"item_reviews\"]))#test item_reviews\n",
    "\n",
    "\n",
    "cnn = CNNRecommend(50, user_seq_len, item_seq_len)#Initialize cnn model\n",
    "cnn.create_cnn_model()\n",
    "cnn.train(train_user_reviews,train_item_reviews, train_rating, epochs=5)#train\n",
    "cnn.evaluate(test_user_reviews,test_item_reviews,test_rating)#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
